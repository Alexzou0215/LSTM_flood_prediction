{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b20932",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install numpy==1.26.4 pandas==2.2.2 matplotlib==3.8.4 scikit-learn==1.4.2 tensorflow==2.16.1 joblib==1.4.2 openpyxl==3.1.2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa4e920b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import os\n",
    "import tensorflow.keras.backend as K\n",
    "import joblib\n",
    "import openpyxl\n",
    "\n",
    "def selective_quantile_loss(tau=0.7, threshold=0.95):\n",
    "    def loss(y_true, y_pred):\n",
    "        mask = K.cast(y_true > threshold, dtype='float32')\n",
    "        error = y_true - y_pred\n",
    "        quantile = K.maximum(tau * error, (tau - 1) * error)\n",
    "        peak_loss = mask * quantile\n",
    "        mse_loss = (1.0 - mask) * K.square(error)\n",
    "        return K.mean(peak_loss + mse_loss)\n",
    "    return loss\n",
    "\n",
    "def load_data(file_path):\n",
    "    df = pd.read_csv(file_path, usecols=[\"Timestamp\", \"Varibale Value\"], low_memory=False)\n",
    "    df['Timestamp'] = pd.to_datetime(df['Timestamp'], format=\"%d/%m/%Y %H:%M\")\n",
    "    df = df.sort_values(by='Timestamp') \n",
    "    df[\"Varibale Value\"] = pd.to_numeric(df[\"Varibale Value\"], errors=\"coerce\")\n",
    "    return df\n",
    "\n",
    "def create_sequences(data, window_size, lead_time, labels=None):\n",
    "    X, Y, seq_labels = [], [], []\n",
    "    n = len(data)\n",
    "    for i in range(n - window_size - lead_time + 1):\n",
    "        if labels is not None:\n",
    "            input_labels = labels[i : i + window_size]\n",
    "            output_labels = labels[i + window_size : i + window_size + lead_time]\n",
    "            all_labels = np.concatenate([input_labels, output_labels])\n",
    "            if not np.all(all_labels == all_labels[0]):\n",
    "                continue\n",
    "            this_label = all_labels[0]\n",
    "        else:\n",
    "            this_label = None\n",
    "        X.append(data[i:i+window_size])\n",
    "        Y.append(data[i+window_size:i+window_size+lead_time].flatten())\n",
    "        if labels is not None:\n",
    "            seq_labels.append(this_label)\n",
    "    if labels is not None:\n",
    "        return np.array(X), np.array(Y), np.array(seq_labels)\n",
    "    else:\n",
    "        return np.array(X), np.array(Y)\n",
    "\n",
    "def preprocess_data_inference(df, scaler, column='Varibale Value', window_size=48, lead_time=16):\n",
    "    scaled_data = scaler.transform(df[[column]])\n",
    "    X, Y = create_sequences(scaled_data, window_size, lead_time)\n",
    "    X = X.reshape((X.shape[0], X.shape[1], 1))\n",
    "    _, Y_unscaled = create_sequences(df[column].values.reshape(-1, 1), window_size, lead_time)\n",
    "    dates_array = df[\"Timestamp\"].values\n",
    "    _, date_sequences = create_sequences(dates_array.reshape(-1, 1), window_size, lead_time)\n",
    "    return X, Y, Y_unscaled, date_sequences\n",
    "\n",
    "def build_lstm_model(window_size, lead_time):\n",
    "    model = Sequential([\n",
    "        LSTM(100, activation='tanh', return_sequences=True, input_shape=(window_size, 1)),\n",
    "        Dropout(0.05),\n",
    "        LSTM(100, activation='tanh'),\n",
    "        Dense(lead_time)\n",
    "    ])\n",
    "    optimizer = Adam(learning_rate=0.001, clipnorm=1.0)\n",
    "    model.compile(optimizer=optimizer, loss=selective_quantile_loss(tau=0.7, threshold=0.95))\n",
    "    return model\n",
    "\n",
    "def forecast(model, X_test, scaler):\n",
    "    predictions = model.predict(X_test)\n",
    "    return scaler.inverse_transform(predictions)\n",
    "\n",
    "def calculate_nse(actual, predicted):\n",
    "    return 1 - (np.sum((actual - predicted) ** 2) / np.sum((actual - np.mean(actual)) ** 2))\n",
    "\n",
    "def calculate_kge(actual, predicted):\n",
    "    r = np.corrcoef(actual.squeeze(), predicted.squeeze())[0, 1]\n",
    "    beta = np.mean(predicted) / np.mean(actual)\n",
    "    gamma = np.std(predicted) / np.std(actual)\n",
    "    return 1 - np.sqrt((r - 1) ** 2 + (beta - 1) ** 2 + (gamma - 1) ** 2)\n",
    "\n",
    "def root_mean_squared_error(actual, predicted):\n",
    "    return np.sqrt(mean_squared_error(actual, predicted))\n",
    "\n",
    "def plot_results(actual, predicted, dates, save_path=\"prediction_vs_actual.png\"):\n",
    "    actual_last    = actual[:, -1]\n",
    "    predicted_last = predicted[:, -1]\n",
    "    dates_last     = dates[:, -1]\n",
    "    rmse       = root_mean_squared_error(actual_last, predicted_last)\n",
    "    nse_value  = calculate_nse(actual_last, predicted_last)\n",
    "    kge_value  = calculate_kge(actual_last, predicted_last)\n",
    "    plt.style.use('ggplot')\n",
    "    plt.rcParams.update({\n",
    "        'font.family':        'sans-serif',\n",
    "        'font.sans-serif':    ['Arial'],\n",
    "        'font.size':         14,\n",
    "        'axes.titlesize':    18,\n",
    "        'axes.labelsize':    16,\n",
    "        'xtick.labelsize':   14,\n",
    "        'ytick.labelsize':   14,\n",
    "        'legend.fontsize':   18,\n",
    "        'figure.titlesize':  20\n",
    "    })\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    fig.patch.set_facecolor('none')    \n",
    "    ax.set_facecolor('none') \n",
    "    ax.grid(False) \n",
    "    ax.plot(dates_last,\n",
    "            actual_last,\n",
    "            label='Actual Flow Rate (ML/day)',\n",
    "            linewidth=2)\n",
    "    ax.plot(dates_last,\n",
    "            predicted_last,\n",
    "            label='Predicted Flow Rate (ML/day)',\n",
    "            linestyle='--',\n",
    "            linewidth=2)\n",
    "    ax.set_title(\n",
    "        f\"RMSE: {rmse:.1f}, NSE: {nse_value:.2f}, KGE: {kge_value:.2f}\"\n",
    "    )\n",
    "    ax.set_xlabel(\"Time\")\n",
    "    ax.set_ylabel(\"Flow Rate (ML/day)\")\n",
    "    ax.legend()\n",
    "    fig.autofmt_xdate(rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path, bbox_inches='tight', dpi=300, transparent=True)\n",
    "    plt.show()\n",
    "    plt.close(fig)\n",
    "\n",
    "def save_results_excel_per_lead(actual, predicted, dates, save_path=\"prediction_results_by_lead.xlsx\"):\n",
    "    n_samples, n_leads = actual.shape\n",
    "    with pd.ExcelWriter(save_path, engine=\"openpyxl\") as writer:\n",
    "        for lead in range(n_leads):\n",
    "            df_lead = pd.DataFrame({\n",
    "                \"Timestamp\": pd.to_datetime(dates[:, lead]),\n",
    "                \"Actual_FlowRate\": actual[:, lead],\n",
    "                \"Predicted_FlowRate\": predicted[:, lead]\n",
    "            })\n",
    "            df_lead.to_excel(writer, sheet_name=f\"Lead_{lead+1}\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc2e8a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- MAIN: All steps in one cell ----\n",
    "file_path = 'test_419012.csv'\n",
    "window_size = 48\n",
    "lead_time = 16\n",
    "\n",
    "model_path = \"lstm_flood_pro10_model2.h5\"\n",
    "weights_path = model_path + \"_best_val.weights.h5\"\n",
    "scaler_path = model_path.replace('.h5', '_scaler.pkl')\n",
    "\n",
    "# 1. Load data\n",
    "df = load_data(file_path)\n",
    "\n",
    "# 2. Load the saved scaler\n",
    "if not os.path.exists(scaler_path):\n",
    "    raise FileNotFoundError(f\"Cannot find scaler file: {scaler_path}\")\n",
    "scaler = joblib.load(scaler_path)\n",
    "\n",
    "# 3. Preprocess new data\n",
    "X_new, Y_new_scaled, Y_new_unscaled, new_dates = preprocess_data_inference(\n",
    "    df, scaler, column='Varibale Value', window_size=window_size, lead_time=lead_time\n",
    ")\n",
    "\n",
    "# 4. Build model architecture (must match trained model)\n",
    "model = build_lstm_model(window_size=window_size, lead_time=lead_time)\n",
    "\n",
    "# 5. Load trained weights\n",
    "if not os.path.exists(weights_path):\n",
    "    raise FileNotFoundError(f\"Cannot find weights file: {weights_path}\")\n",
    "model.load_weights(weights_path)\n",
    "\n",
    "# 6. Make predictions\n",
    "Y_new_predictions = forecast(model, X_new, scaler)\n",
    "\n",
    "# 7. Plot and save figure\n",
    "plot_results(Y_new_unscaled, Y_new_predictions, new_dates, save_path=\"prediction_vs_actual.png\")\n",
    "\n",
    "# 8. Save predictions and actuals for each lead as Excel\n",
    "save_results_excel_per_lead(Y_new_unscaled, Y_new_predictions, new_dates, save_path=\"prediction_results_by_lead.xlsx\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lstm_flood_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
